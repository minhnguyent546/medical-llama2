# BERTSCores for Alpaca-Llama-2-7b-chat

## MedInstruct-test

### gpt-4 answer

bert_score: precision = 0.292, recall = 0.233, F1 = 0.263 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.881, recall = 0.871, F1 = 0.876 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### gpt-3.5-turbo answer

bert_score: precision = 0.328, recall = 0.280, F1 = 0.304 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.887, recall = 0.879, F1 = 0.883 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### text-davinci-003 answer

bert_score: precision = 0.206, recall = 0.409, F1 = 0.306 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.866, recall = 0.900, F1 = 0.883 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### claude-2 answer

bert_score: precision = 0.258, recall = 0.179, F1 = 0.219 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.875, recall = 0.862, F1 = 0.868 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

## iCliniq-1k

### gpt-4 answer

bert_score: precision = 0.228, recall = 0.168, F1 = 0.198 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.870, recall = 0.860, F1 = 0.865 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### gpt-3.5-turbo answer

bert_score: precision = 0.277, recall = 0.260, F1 = 0.269 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.878, recall = 0.875, F1 = 0.877 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### text-davinci-003 answer

bert_score: precision = 0.152, recall = 0.362, F1 = 0.256 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.857, recall = 0.892, F1 = 0.874 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### claude-2 answer

bert_score: precision = 0.181, recall = 0.129, F1 = 0.155 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.862, recall = 0.853, F1 = 0.857 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer
