# BERTSCores for Med-Alpaca-2-7b-chat

## MedInstruct-test

### gpt-4 answer

bert_score: precision = 0.298, recall = 0.260, F1 = 0.279 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.882, recall = 0.875, F1 = 0.878 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### gpt-3.5-turbo answer

bert_score: precision = 0.365, recall = 0.336, F1 = 0.350 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.893, recall = 0.888, F1 = 0.890 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### text-davinci-003 answer

bert_score: precision = 0.167, recall = 0.393, F1 = 0.278 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.860, recall = 0.898, F1 = 0.878 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### claude-2 answer

bert_score: precision = 0.248, recall = 0.191, F1 = 0.220 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.873, recall = 0.864, F1 = 0.868 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

## iCliniq-1k

### gpt-4 answer

bert_score: precision = 0.205, recall = 0.199, F1 = 0.203 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.866, recall = 0.865, F1 = 0.865 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### gpt-3.5-turbo answer

bert_score: precision = 0.267, recall = 0.313, F1 = 0.290 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.876, recall = 0.884, F1 = 0.880 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### text-davinci-003 answer

bert_score: precision = 0.089, recall = 0.351, F1 = 0.217 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.846, recall = 0.891, F1 = 0.868 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer

### claude-2 answer

bert_score: precision = 0.158, recall = 0.159, F1 = 0.159 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)-rescaled_fast-tokenizer
bert_score_unscaled: precision = 0.858, recall = 0.858, F1 = 0.858 hash_code = roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.3)_fast-tokenizer
